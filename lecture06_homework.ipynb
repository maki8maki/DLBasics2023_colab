{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maki8maki/DLBasics2023_colab/blob/master/lecture06_homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI0CCRYGbsLX"
      },
      "source": [
        "# 第6回講義 宿題"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhZjl_2gbsLf"
      },
      "source": [
        "## 課題\n",
        "RNNを用いてIMDbのsentiment analysisを実装してみましょう．\n",
        "\n",
        "ネットワークの形などに制限はとくになく，今回のLessonで扱った内容以外の工夫も組み込んでもらって構いません．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBcfs2VybsLj"
      },
      "source": [
        "## 目標値\n",
        "F値：0.85"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZj-U7SvbsLl"
      },
      "source": [
        "## ルール\n",
        "- 以下のセルで指定されている`x_train`, `t_train`以外の学習データは使わないでください．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm9ZJX2TbsLo"
      },
      "source": [
        "## 提出方法\n",
        "- 2つのファイルを提出していただきます．\n",
        "  1. テストデータ `x_test` に対する予測ラベルを`submission_pred.csv`として保存し，Omnicampusの宿題から「第6回 回帰結合型ニューラルネットワーク」を選択して提出してください．\n",
        "  2. それに対応するpythonのコードを`submission_code.py`として保存し，Omnicampusの宿題から「第6回 回帰結合型ニューラルネットワーク (code)」を選択して提出してください．\n",
        "    - セルに書いたコードを.py形式で保存するためには%%writefileコマンドなどを利用してください．\n",
        "    - writefileコマンドではファイルの保存のみが行われセル内のpythonコード自体は実行されません．そのため，実際にコードを走らせる際にはwritefileコマンドをコメントアウトしてください．\n",
        "\n",
        "\n",
        "- コードの内容を変更した場合は，1と2の両方を提出し直してください．\n",
        "\n",
        "- なお採点は1で行い，2はコードの確認用として利用します．(成績優秀者はコード内容を公開させていただくかもしれません)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejdA6CESbsLs"
      },
      "source": [
        "## 評価方法\n",
        "\n",
        "- 予測ラベルの`t_test`に対するF値で評価します．\n",
        "- 即時採点しLeader Boardを更新します．（採点スケジュールは別アナウンス）\n",
        "- 締切時の点数を最終的な評価とします．\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyYaymAQ4Wey"
      },
      "source": [
        "### ドライブのマウント"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Szesnftz4a1K",
        "outputId": "aa550def-b2f6-466f-bc9f-3b08f0350834"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjSgBf1ibsLw"
      },
      "source": [
        "## データの読み込み（このセルは修正しないでください）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4X9ptijybsLz",
        "outputId": "1078deed-1289-4bae-986e-9c7d0e8af3b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (2.7.0)\n",
            "単語種数: 88587\n"
          ]
        }
      ],
      "source": [
        "!pip install portalocker\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import vocab\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "from typing import List, Union\n",
        "\n",
        "seed = 1234\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "\n",
        "# 学習データ\n",
        "x_train = np.load('drive/MyDrive/Colab Notebooks/DLBasics2023_colab/Lecture06/data/x_train.npy', allow_pickle=True)\n",
        "t_train = np.load('drive/MyDrive/Colab Notebooks/DLBasics2023_colab/Lecture06/data/t_train.npy', allow_pickle=True)\n",
        "\n",
        "# 検証データを取る\n",
        "x_train, x_valid, t_train, t_valid = train_test_split(x_train, t_train, test_size=0.2, random_state=seed)\n",
        "    \n",
        "# テストデータ\n",
        "x_test = np.load('drive/MyDrive/Colab Notebooks/DLBasics2023_colab/Lecture06/data/x_test.npy', allow_pickle=True)\n",
        "\n",
        "\n",
        "def text_transform(text: List[int], max_length=256):\n",
        "    # <BOS>はすでに1で入っている．<EOS>は2とする．\n",
        "    text = text[:max_length - 1] + [2]\n",
        "\n",
        "    return text, len(text)\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, len_seq_list = [], [], []\n",
        "    \n",
        "    for sample in batch:\n",
        "        if isinstance(sample, tuple):\n",
        "            label, text = sample\n",
        "\n",
        "            label_list.append(label)\n",
        "        else:\n",
        "            text = sample.copy()\n",
        "            \n",
        "        text, len_seq = text_transform(text)\n",
        "        text_list.append(torch.tensor(text))\n",
        "        len_seq_list.append(len_seq)\n",
        "        \n",
        "    # NOTE: 宿題用データセットでは<PAD>は3です．\n",
        "    return torch.tensor(label_list), pad_sequence(text_list, padding_value=3).T, torch.tensor(len_seq_list)\n",
        "\n",
        "\n",
        "word_num = np.concatenate(np.concatenate((x_train, x_test))).max() + 1\n",
        "print(f\"単語種数: {word_num}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIKgr01wbsL5"
      },
      "source": [
        "## 実装"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    [(t, x) for t, x in zip(t_train, x_train)],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_batch,\n",
        ")\n",
        "valid_dataloader = DataLoader(\n",
        "    [(t, x) for t, x in zip(t_valid, x_valid)],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_batch,\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    x_test,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_batch,\n",
        ")"
      ],
      "metadata": {
        "id": "WPhmbQtagOvH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "w6j2soNLbsL7"
      },
      "outputs": [],
      "source": [
        "def torch_log(x):\n",
        "    return torch.log(torch.clamp(x, min=1e-10))\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, hid_dim, da, r):\n",
        "    super(SelfAttention, self).__init__()\n",
        "    self.hid_dim = hid_dim\n",
        "    self.da = da\n",
        "    self.r = r\n",
        "    self.main = nn.Sequential(\n",
        "        nn.Linear(hid_dim * 2, da), \n",
        "        nn.ReLU(),\n",
        "        nn.Linear(da, r)\n",
        "    )\n",
        "  def forward(self, out):\n",
        "    return F.softmax(self.main(out), dim=1)\n",
        "\n",
        "class NetWithSelfAttention(nn.Module):\n",
        "    def __init__(self, word_num, emb_dim, hid_dim, da):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(word_num, emb_dim)\n",
        "        self.bigru = nn.GRU(emb_dim, hid_dim, batch_first=True, bidirectional=True, num_layers=2, dropout=0.25)\n",
        "        self.attn = SelfAttention(hid_dim, da, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.25)\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(hid_dim * 2, hid_dim), \n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hid_dim, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x, len_seq_max=0, len_seq=None, init_state=None):\n",
        "        h = self.dropout1(self.emb(x)) # (batch_size, seq_length, emb_dim)\n",
        "\n",
        "        if len_seq_max > 0:\n",
        "            out, h = self.bigru(h[:, 0:len_seq_max, :], init_state)\n",
        "        else:\n",
        "            out, h = self.bigru(h, init_state) # (batch_size, seq_length, hid_dim*2)\n",
        "        \n",
        "        attn_weight = self.attn(out)\n",
        "        feats = (out * attn_weight).unsqueeze(2).sum(dim=1)\n",
        "        \n",
        "        return self.main(self.dropout2(feats))\n",
        "\n",
        "class SequenceTaggingNet(nn.Module):\n",
        "    def __init__(self, word_num, emb_dim, hid_dim):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(word_num, emb_dim)\n",
        "        self.bigru = nn.GRU(emb_dim, hid_dim, batch_first=True, bidirectional=True, num_layers=2, dropout=0.25)\n",
        "        self.linear = nn.Linear(hid_dim*2, 1) # ForwardとBackwardの出力をconcatしたものを渡すので2倍\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.25)\n",
        "    \n",
        "    def forward(self, x, len_seq_max=0, len_seq=None, init_state=None):\n",
        "        h = self.dropout1(self.emb(x)) # (batch_size, seq_length, emb_dim)\n",
        "\n",
        "        if len_seq_max > 0:\n",
        "            _, hc = self.bigru(h[:, 0:len_seq_max, :], init_state)\n",
        "        else:\n",
        "            _, hc = self.bigru(h, init_state) # (batch_size, seq_length, hid_dim*2)\n",
        "        \n",
        "        out = torch.cat((hc[-2,:,:], hc[-1,:,:]), dim=1)\n",
        "        print(out.size())\n",
        "        y = self.linear(self.dropout2(out))\n",
        "        \n",
        "        return y\n",
        "\n",
        "def calc_loss(y, t):\n",
        "    return -torch.mean(t * torch_log(y) + (1-t) * torch_log(1-y))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb_dim = 200\n",
        "hid_dim = 128\n",
        "da = 64\n",
        "n_epochs = 50\n",
        "device = 'cuda'\n",
        "\n",
        "# net = SequenceTaggingNet(word_num, emb_dim, hid_dim)\n",
        "net = NetWithSelfAttention(word_num, emb_dim, hid_dim, da)\n",
        "net.to(device)\n",
        "optimizer = optim.Adam(net.parameters())\n",
        "\n",
        "min_loss = torch.inf\n",
        "patience = 10 # 許容できる非改善エポック数\n",
        "n_worsening = 0 # 連続非改善エポック数\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    losses_train = []\n",
        "    losses_valid = []\n",
        "\n",
        "    net.train()\n",
        "    n_train = 0\n",
        "    acc_train = 0\n",
        "    for label, line, len_seq in train_dataloader:\n",
        "        net.zero_grad()\n",
        "\n",
        "        t = label.to(device)\n",
        "        x = line.to(device) # (batch, time)\n",
        "        len_seq.to(device)\n",
        "\n",
        "        h = net(x, torch.max(len_seq), len_seq)\n",
        "        y = torch.sigmoid(h).squeeze()\n",
        "\n",
        "        loss = calc_loss(y, t)\n",
        "        loss.backward()\n",
        "\n",
        "        # 勾配を絶対値1.0でクリッピングする\n",
        "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        losses_train.append(loss.tolist())\n",
        "\n",
        "        n_train += t.size()[0]\n",
        "\n",
        "    # Valid\n",
        "    t_valid = []\n",
        "    y_pred = []\n",
        "    net.eval()\n",
        "    for label, line, len_seq in valid_dataloader:\n",
        "        t = label.to(device)\n",
        "        x = line.to(device)\n",
        "        len_seq.to(device)\n",
        "\n",
        "        h = net(x, torch.max(len_seq), len_seq)\n",
        "        y = torch.sigmoid(h).squeeze()\n",
        "\n",
        "        loss = calc_loss(y, t)\n",
        "        pred = y.round().squeeze()\n",
        "\n",
        "        t_valid.extend(t.tolist())\n",
        "        y_pred.extend(pred.tolist())\n",
        "\n",
        "        losses_valid.append(loss.tolist())\n",
        "\n",
        "    if np.mean(losses_valid) < min_loss:\n",
        "        min_loss = np.mean(losses_valid)\n",
        "        state_dict = net.state_dict()\n",
        "        n_worsening = 0\n",
        "    else:\n",
        "        n_worsening += 1\n",
        "    print('EPOCH: {}, Train Loss: {:.3f}, Valid Loss: {:.3f}, Validation F1: {:.3f}'.format(\n",
        "        epoch,\n",
        "        np.mean(losses_train),\n",
        "        np.mean(losses_valid),\n",
        "        f1_score(t_valid, y_pred, average='macro')\n",
        "    ))\n",
        "    if n_worsening >= patience:\n",
        "        net.load_state_dict(state_dict)\n",
        "        break"
      ],
      "metadata": {
        "id": "9w3yjqqCl_Zb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eda3d706-a2ff-494e-88ba-aafe25930fe4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0, Train Loss: 0.487, Valid Loss: 0.331, Validation F1: 0.857\n",
            "EPOCH: 1, Train Loss: 0.301, Valid Loss: 0.287, Validation F1: 0.881\n",
            "EPOCH: 2, Train Loss: 0.231, Valid Loss: 0.292, Validation F1: 0.884\n",
            "EPOCH: 3, Train Loss: 0.191, Valid Loss: 0.291, Validation F1: 0.894\n",
            "EPOCH: 4, Train Loss: 0.144, Valid Loss: 0.352, Validation F1: 0.884\n",
            "EPOCH: 5, Train Loss: 0.111, Valid Loss: 0.347, Validation F1: 0.890\n",
            "EPOCH: 6, Train Loss: 0.087, Valid Loss: 0.352, Validation F1: 0.893\n",
            "EPOCH: 7, Train Loss: 0.068, Valid Loss: 0.385, Validation F1: 0.892\n",
            "EPOCH: 8, Train Loss: 0.051, Valid Loss: 0.449, Validation F1: 0.895\n",
            "EPOCH: 9, Train Loss: 0.045, Valid Loss: 0.488, Validation F1: 0.889\n",
            "EPOCH: 10, Train Loss: 0.038, Valid Loss: 0.485, Validation F1: 0.892\n",
            "EPOCH: 11, Train Loss: 0.031, Valid Loss: 0.548, Validation F1: 0.892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "dQ8xtUcHj6IR"
      },
      "outputs": [],
      "source": [
        "net.eval()\n",
        "\n",
        "y_pred = []\n",
        "for _, line, len_seq in test_dataloader:\n",
        "\n",
        "    x = line.to(device)\n",
        "    len_seq.to(device)\n",
        "\n",
        "    h = net(x, torch.max(len_seq), len_seq)\n",
        "    y = torch.sigmoid(h).squeeze()\n",
        "\n",
        "    pred = y.round().squeeze()  # 0.5以上の値を持つ要素を正ラベルと予測する\n",
        "\n",
        "    y_pred.extend(pred.tolist())\n",
        "\n",
        "\n",
        "submission = pd.Series(y_pred, name='label')\n",
        "submission.to_csv('drive/MyDrive/Colab Notebooks/DLBasics2023_colab/Lecture06/submission_pred.csv', header=True, index_label='id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-MRCNboHZNd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}