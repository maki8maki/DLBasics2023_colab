{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maki8maki/DLBasics2023_colab/blob/master/lecture09_homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUU-McVcFGJ0"
      },
      "source": [
        "# 第9回講義 宿題\n",
        "\n",
        "## 課題\n",
        "自己教師あり学習を用いて事前学習を行い，得られた表現をLinear probingで評価してみましょう．  \n",
        "ネットワークの形などに制限はとくになく，今回のLessonで扱った内容以外の工夫も組み込んでもらって構いません．   \n",
        "\n",
        "## 目標精度\n",
        "なし\n",
        "- 自己教師あり学習の手法によっては計算リソースによって性能が大きく変わるため，目標精度は設定しておりません．\n",
        "- ただし以下の工夫を行うことで計算リソースが少なくとも，長い学習を分割して行うことができます．  \n",
        "    - model，optimizer, schedulerを一定エポックで保存して，読み込むことで学習を再開することができます．\n",
        "    - 演習のようにschedulerを実装した場合は，保存は必要なく同じ引数でインスタンスを作成して\\_\\_call\\_\\_の際に与えるepochを学習の続きから与えれば動作します．  \n",
        "    - 参考: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
        "\n",
        "## ルール\n",
        "- 予測ラベルは one_hot表現ではなく0~9のクラスラベル で表してください．\n",
        "- 自己教師あり学習では以下のセルで指定されている`x_train`以外の学習データは用いないでください．\n",
        "- Linear probingの際には`x_train`, `t_train`以外の学習データは用いないでください．\n",
        "\n",
        "## 提出方法\n",
        "- 2つのファイルを提出していただきます．\n",
        "    - テストデータ (x_test) に対する予測ラベルをcsvファイル (ファイル名: submission_pred.csv) で提出してください．\n",
        "    - それに対応するpythonのコードをsubmission_code.pyとして提出してください (%%writefileコマンドなどを利用してください)．\n",
        "\n",
        "- コードの内容を変更した場合は，1と2の両方を提出し直してください．\n",
        "\n",
        "- なお採点は1で行い，2はコードの確認用として利用します．(成績優秀者はコード内容を公開させていただくかもしれません)\n",
        "\n",
        "## 評価方法\n",
        "\n",
        "- 予測ラベルの`t_test`に対するAccuracyで評価します．\n",
        "- 即時採点しLeader Boardを更新します．\n",
        "- 締切時の点数を最終的な評価とします．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtPxAOyKHN_l"
      },
      "source": [
        "### ドライブのマウント"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UQM_SpiDHfso",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c31591f2-1dec-470f-beeb-9082115b0da3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN8Jq5a8J5HV"
      },
      "source": [
        "### データの読み込み\n",
        "- この部分は修正しないでください．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LNpUF5xOJ8bG"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#学習データ\n",
        "x_train = np.load('drive/MyDrive/Colab Notebooks/DLBasics2023_colab/Lecture09/data/x_train.npy')\n",
        "t_train = np.load('drive/MyDrive/Colab Notebooks/DLBasics2023_colab/Lecture09/data/t_train.npy')\n",
        "\n",
        "#テストデータ\n",
        "x_test = np.load('drive/MyDrive/Colab Notebooks/DLBasics2023_colab/Lecture09/data/x_test.npy')\n",
        "\n",
        "class train_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, x_train, t_train):\n",
        "        data = x_train.astype('float32')\n",
        "        self.x_train = []\n",
        "        for i in range(data.shape[0]):\n",
        "            self.x_train.append(Image.fromarray(np.uint8(data[i])))\n",
        "        self.t_train = t_train\n",
        "        self.transform = transforms.ToTensor()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_train)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.transform(self.x_train[idx]), torch.tensor(t_train[idx], dtype=torch.long)\n",
        "\n",
        "class test_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, x_test):\n",
        "        data = x_test.astype('float32')\n",
        "        self.x_test = []\n",
        "        for i in range(data.shape[0]):\n",
        "            self.x_test.append(Image.fromarray(np.uint8(data[i])))\n",
        "        self.transform = transforms.ToTensor()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_test)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.transform(self.x_test[idx])\n",
        "\n",
        "trainval_data = train_dataset(x_train, t_train)\n",
        "test_data = test_dataset(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSqA6Ni3MDSX"
      },
      "source": [
        "### データローダの準備  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "63ODMwChMEy_"
      },
      "outputs": [],
      "source": [
        "val_size = 3000\n",
        "train_data, valid_data = torch.utils.data.random_split(trainval_data, [len(trainval_data) - val_size, val_size])\n",
        "\n",
        "train_transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize(0.5, 0.5)]\n",
        ")\n",
        "valid_transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize(0.5, 0.5)]\n",
        ")\n",
        "\n",
        "test_transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize(0.5, 0.5)]\n",
        ")\n",
        "\n",
        "train_data.transform = train_transform\n",
        "valid_data.transform = valid_transform\n",
        "test_data.transform = test_transform\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "dataloader_train = torch.utils.data.DataLoader(\n",
        "    train_data,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "dataloader_valid = torch.utils.data.DataLoader(\n",
        "    valid_data,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "dataloader_test = torch.utils.data.DataLoader(\n",
        "    test_data,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQpTXlwbKRdW"
      },
      "source": [
        "### 自己教師あり学習の実装\n",
        "- 初期の形式はMAEを利用することを想定していますが，他の自己教師あり学習を利用していただいて構いません．   "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrrtNpM9fkt1",
        "outputId": "0d4996ab-7815-4a09-c6e9-68bcd615d3de"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from einops.layers.torch import Rearrange\n",
        "from einops import rearrange"
      ],
      "metadata": {
        "id": "PzjDGsuofQR7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####主要な関数の実装"
      ],
      "metadata": {
        "id": "WPpHirQCkbHe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TzlJ4q1uKagF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66606d0c-1e37-4483-c141-85b746657414"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "def fix_seed(seed=1234):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "\n",
        "fix_seed(seed=42)\n",
        "\n",
        "\n",
        "def random_indexes(size):\n",
        "    \"\"\"\n",
        "    パッチをランダムに並べ替えるためのindexを生成する関数．\n",
        "\n",
        "    Argument\n",
        "    --------\n",
        "    size : int\n",
        "        入力されるパッチの数（系列長Nと同じ値）．\n",
        "    \"\"\"\n",
        "    forward_indexes = np.arange(size)  # 0からsizeまでを並べた配列を作成\n",
        "    np.random.shuffle(forward_indexes)  # 生成した配列をシャッフルすることで，パッチの順番をランダムに決定\n",
        "    backward_indexes = np.argsort(forward_indexes)  # 並べ替えたパッチをもとの順番に戻すためのidx\n",
        "\n",
        "    return forward_indexes, backward_indexes\n",
        "\n",
        "\n",
        "def take_indexes(sequences, indexes):\n",
        "    \"\"\"\n",
        "    パッチを並べ替えるための関数．\n",
        "\n",
        "    Argument\n",
        "    --------\n",
        "    sequences : torch.Tensor\n",
        "        入力画像をパッチ分割したデータ．(B, N, dim)の形状をしている．\n",
        "    indexes : np.ndarray\n",
        "        並べ替えるために利用するindex．\n",
        "        random_indexesで生成したforward_indexesかbackward_indexesが入ることが想定されている．\n",
        "    \"\"\"\n",
        "    return torch.gather(sequences, dim=1, index=indexes.unsqueeze(2).repeat(1, 1, sequences.shape[-1]))\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads, dim_head, dropout=0.):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        dim : int\n",
        "            入力データの次元数．埋め込み次元数と一致する．\n",
        "        heads : int\n",
        "            ヘッドの数．\n",
        "        dim_head : int\n",
        "            各ヘッドのデータの次元数．\n",
        "        dropout : float\n",
        "            Dropoutの確率(default=0.)．\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.dim_head = dim_head\n",
        "        inner_dim = dim_head * heads  # ヘッドに分割する前のQ, K, Vの次元数．self.dimと異なっても良い．\n",
        "        project_out = not (heads == 1 and dim_head == dim)  # headsが1，dim_headがdimと等しければ通常のSelf-Attention\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = math.sqrt(dim_head)  # ソフトマックス関数を適用する前のスケーリング係数(dim_k)\n",
        "\n",
        "        self.attend = nn.Softmax(dim=-1)  # アテンションスコアの算出に利用するソフトマックス関数\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Q, K, Vに変換するための全結合層\n",
        "        self.to_q = nn.Linear(in_features=dim, out_features=inner_dim)\n",
        "        self.to_k = nn.Linear(in_features=dim, out_features=inner_dim)\n",
        "        self.to_v = nn.Linear(in_features=dim, out_features=inner_dim)\n",
        "\n",
        "        # dim != inner_dimなら線形層を入れる，そうでなければそのまま出力\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(in_features=inner_dim, out_features=dim),\n",
        "            nn.Dropout(dropout),\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        B: バッチサイズ\n",
        "        N: 系列長\n",
        "        D: データの次元数(dim)\n",
        "        \"\"\"\n",
        "        B, N, D = x.size()\n",
        "\n",
        "        # 入力データをQ, K, Vに変換する\n",
        "        # (B, N, dim) -> (B, N, inner_dim)\n",
        "        q = self.to_q(x)\n",
        "        k = self.to_k(x)\n",
        "        v = self.to_v(x)\n",
        "\n",
        "        # Q, K, Vをヘッドに分割する\n",
        "        # (B, N, inner_dim) -> (B, heads, N, dim_head)\n",
        "        q = rearrange(q, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n",
        "        k = rearrange(k, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n",
        "        v = rearrange(v, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n",
        "\n",
        "        # QK^T / sqrt(d_k)を計算する\n",
        "        # (B, heads, N, dim_head) x (B, heads, dim_head, N) -> (B, heads, N, N)\n",
        "        dots = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
        "\n",
        "        # ソフトマックス関数でスコアを算出し，Dropoutをする\n",
        "        attn = self.attend(dots)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # softmax(QK^T / sqrt(d_k))Vを計算する\n",
        "        # (B, heads, N, N) x (B, heads, N, dim_head) -> (B, heads, N, dim_head)\n",
        "        out = torch.matmul(attn ,v)\n",
        "\n",
        "        # もとの形に戻す\n",
        "        # (B, heads, N, dim_head) -> (B, N, dim)\n",
        "        out = rearrange(out, \"b h n d -> b n (h d)\", h=self.heads, d=self.dim_head)\n",
        "\n",
        "        # 次元が違っていればもとに戻して出力\n",
        "        # 表現の可視化のためにattention mapも返すようにしておく\n",
        "        return self.to_out(out), attn\n",
        "\n",
        "\n",
        "class FFN(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        dim : int\n",
        "            入力データの次元数．\n",
        "        hidden_dim : int\n",
        "            隠れ層の次元．\n",
        "        dropout : float\n",
        "            各全結合層の後のDropoutの確率(default=0.)．\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_features=dim, out_features=hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(in_features=hidden_dim, out_features=dim),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        (B, D) -> (B, D)\n",
        "        B: バッチサイズ\n",
        "        D: 次元数\n",
        "        \"\"\"\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, heads, dim_head, mlp_dim, dropout):\n",
        "        \"\"\"\n",
        "        TransformerのEncoder Blockの実装．\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        dim : int\n",
        "            埋め込みされた次元数．PatchEmbedのembed_dimと同じ値．\n",
        "        heads : int\n",
        "            Multi-Head Attentionのヘッドの数．\n",
        "        dim_head : int\n",
        "            Multi-Head Attentionの各ヘッドの次元数．\n",
        "        mlp_dim : int\n",
        "            Feed-Forward Networkの隠れ層の次元数．\n",
        "        dropout : float\n",
        "            Droptou層の確率p．\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.attn_ln = nn.LayerNorm(dim)  # Attention前のLayerNorm\n",
        "        self.attn = Attention(dim, heads, dim_head, dropout)\n",
        "        self.ffn_ln = nn.LayerNorm(dim)  # FFN前のLayerNorm\n",
        "        self.ffn = FFN(dim, mlp_dim, dropout)\n",
        "\n",
        "    def forward(self, x, return_attn=False):\n",
        "        \"\"\"\n",
        "        x: (B, N, dim)\n",
        "        B: バッチサイズ\n",
        "        N: 系列長\n",
        "        dim: 埋め込み次元\n",
        "        \"\"\"\n",
        "        y, attn = self.attn(self.attn_ln(x))\n",
        "        if return_attn:  # attention mapを返す（attention mapの可視化に利用）\n",
        "            return attn\n",
        "        x = y + x\n",
        "        out = self.ffn(self.ffn_ln(x)) + x\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
        "        \"\"\"\n",
        "        入力画像をパッチごとに埋め込むための層．\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        image_size : Tuple[int]\n",
        "            入力画像のサイズ．\n",
        "        patch_size : Tuple[int]\n",
        "            各パッチのサイズ．\n",
        "        in_channels : int\n",
        "            入力画像のチャネル数．\n",
        "        embed_dim : int\n",
        "            埋め込み後の次元数．\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        image_height, image_width = image_size\n",
        "        patch_height, patch_width = patch_size\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, \"パッチサイズは，入力画像のサイズを割り切れる必要があります．\"\n",
        "\n",
        "        num_patches = (image_height // patch_height) * (image_width // patch_width)  # パッチの数\n",
        "        patch_dim = in_channels * patch_height * patch_width  # 各パッチを平坦化したときの次元数\n",
        "\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange(\"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1=patch_height, p2=patch_width),  # 画像をパッチに分割して平坦化\n",
        "            nn.Linear(in_features=patch_dim, out_features=embed_dim),  # 埋め込みを行う\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        B: バッチサイズ\n",
        "        C: 入力画像のチャネル数\n",
        "        H: 入力画像の高さ\n",
        "        W: 入力画像の幅\n",
        "        \"\"\"\n",
        "        return self.to_patch_embedding(x)  # (B, C, H, W) -> (B, num_patches, embed_dim)\n",
        "\n",
        "\n",
        "class PatchShuffle(nn.Module):\n",
        "    def __init__(self, ratio):\n",
        "        # ratio: Encoderに入力しないパッチの割合\n",
        "        super().__init__()\n",
        "        self.ratio = ratio\n",
        "\n",
        "    def forward(self, patches):\n",
        "        \"\"\"\n",
        "        B: バッチサイズ\n",
        "        N: 系列長（＝パッチの数）\n",
        "        dim: 次元数（＝埋め込みの次元数）\n",
        "        \"\"\"\n",
        "        B, N, dim = patches.shape\n",
        "        remain_N = int(N * (1 - self.ratio))  # Encoderに入力するパッチの数\n",
        "\n",
        "        indexes = [random_indexes(N) for _ in range(B)]  # バッチごとに異なる順番のindexを作る\n",
        "        forward_indexes = torch.as_tensor(np.stack([i[0] for i in indexes], axis=-1), dtype=torch.long).T.to(patches.device)  # バッチを並べ替えるときのidx (B, N)\n",
        "        backward_indexes = torch.as_tensor(np.stack([i[1] for i in indexes], axis=-1), dtype=torch.long).T.to(patches.device)  # 並べ替えたパッチをもとの順番に戻すためのidx  (B, N)\n",
        "\n",
        "        patches = take_indexes(patches, forward_indexes)  # パッチを並べ替える\n",
        "        patches = patches[:, :remain_N, :]  # Encoderに入力するパッチを抽出\n",
        "\n",
        "        return patches, forward_indexes, backward_indexes\n",
        "\n",
        "\n",
        "class MAE_Encoder(nn.Module):\n",
        "    def __init__(self, image_size=[32, 32], patch_size=[2, 2], emb_dim=192, num_layer=12,\n",
        "                 heads=3, dim_head=64, mlp_dim=192, mask_ratio=0.75, dropout=0.):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "\n",
        "        image_size : List[int]\n",
        "            入力画像の大きさ．\n",
        "        patch_size : List[int]\n",
        "            各パッチの大きさ．\n",
        "        emb_dim : int\n",
        "            データを埋め込む次元の数．\n",
        "        num_layer : int\n",
        "            Encoderに含まれるBlockの数．\n",
        "        heads : int\n",
        "            Multi-Head Attentionのヘッドの数．\n",
        "        dim_head : int\n",
        "            Multi-Head Attentionの各ヘッドの次元数．\n",
        "        mlp_dim : int\n",
        "            Feed-Forward Networkの隠れ層の次元数．\n",
        "        mask_ratio : float\n",
        "            入力パッチのマスクする割合．\n",
        "        dropout : float\n",
        "            ドロップアウトの確率．\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        img_height, img_width = image_size\n",
        "        patch_height, patch_width = patch_size\n",
        "        num_patches = (img_height // patch_height) * (img_width // patch_width)\n",
        "\n",
        "        self.cls_token = torch.nn.Parameter(torch.randn(1, 1, emb_dim))  # class tokenの初期化\n",
        "        self.pos_embedding = torch.nn.Parameter(torch.randn(1, num_patches, emb_dim))  # positional embedding（学習可能にしている）\n",
        "        self.shuffle = PatchShuffle(mask_ratio)\n",
        "\n",
        "        # 入力画像をパッチに分割する\n",
        "        self.patchify = PatchEmbedding(image_size, patch_size, 3, emb_dim)\n",
        "\n",
        "        # Encoder（Blockを重ねる）\n",
        "        self.transformer = torch.nn.Sequential(*[Block(emb_dim, heads, dim_head, mlp_dim, dropout) for _ in range(num_layer)])\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(emb_dim)\n",
        "\n",
        "        self.init_weight()\n",
        "\n",
        "    def init_weight(self):\n",
        "        torch.nn.init.normal_(self.cls_token, std=0.02)\n",
        "        torch.nn.init.normal_(self.pos_embedding, std=0.02)\n",
        "\n",
        "    def forward(self, img):\n",
        "        # 1. 入力画像をパッチに分割して，positional embeddingする\n",
        "        patches = self.patchify(img)\n",
        "        patches = patches + self.pos_embedding\n",
        "\n",
        "        # 2. 分割したパッチをランダムに並べ替えて，必要なパッチのみ得る\n",
        "        patches, forward_indexes, backward_indexes = self.shuffle(patches)\n",
        "\n",
        "        # class tokenを結合\n",
        "        patches = torch.cat([self.cls_token.repeat(patches.shape[0], 1, 1), patches], dim=1)\n",
        "\n",
        "        # 3. Encoderで入力データを処理する\n",
        "        features = self.layer_norm(self.transformer(patches))\n",
        "\n",
        "        return features, backward_indexes\n",
        "\n",
        "\n",
        "class MAE_Decoder(nn.Module):\n",
        "    def __init__(self, image_size=[32, 32], patch_size=[2, 2], emb_dim=192, num_layer=4,\n",
        "                 heads=3, dim_head=64, mlp_dim=192, dropout=0.):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "\n",
        "        image_size : List[int]\n",
        "            入力画像の大きさ．\n",
        "        patch_size : List[int]\n",
        "            各パッチの大きさ．\n",
        "        emb_dim : int\n",
        "            データを埋め込む次元の数．\n",
        "        num_layer : int\n",
        "            Decoderに含まれるBlockの数．\n",
        "        heads : int\n",
        "            Multi-Head Attentionのヘッドの数．\n",
        "        dim_head : int\n",
        "            Multi-Head Attentionの各ヘッドの次元数．\n",
        "        mlp_dim : int\n",
        "            Feed-Forward Networkの隠れ層の次元数．\n",
        "        dropout : float\n",
        "            ドロップアウトの確率．\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        img_height, img_width = image_size\n",
        "        patch_height, patch_width = patch_size\n",
        "        num_patches = (img_height // patch_height) * (img_width // patch_width)\n",
        "\n",
        "        self.mask_token = torch.nn.Parameter(torch.rand(1, 1, emb_dim))\n",
        "        self.pos_embedding = torch.nn.Parameter(torch.rand(1, num_patches+1, emb_dim))\n",
        "\n",
        "        # Decoder(Blockを重ねる）\n",
        "        self.transformer = torch.nn.Sequential(*[Block(emb_dim, heads, dim_head, mlp_dim, dropout) for _ in range(num_layer)])\n",
        "\n",
        "        # 埋め込みされた表現から画像を復元するためのhead\n",
        "        self.head = torch.nn.Linear(emb_dim, 3 * patch_height * patch_width)\n",
        "        # (B, N, dim)から(B, C, H, W)にreshapeするためのインスタンス\n",
        "        self.patch2img = Rearrange(\"b (h w) (c p1 p2) -> b c (h p1) (w p2)\", p1=patch_height, p2=patch_width, h=img_height // patch_height)\n",
        "\n",
        "        self.init_weight()\n",
        "\n",
        "    def init_weight(self):\n",
        "        torch.nn.init.normal_(self.mask_token, std=0.02)\n",
        "        torch.nn.init.normal_(self.pos_embedding, std=0.02)\n",
        "\n",
        "    def forward(self, features, backward_indexes):\n",
        "        # 系列長\n",
        "        T = features.shape[1]\n",
        "\n",
        "        # class tokenがある分backward_indexesの最初に0を追加する\n",
        "        # .toはデバイスの変更でよく利用するが，tensorを渡すことでdtypeを変えることができる\n",
        "        backward_indexes = torch.cat([torch.zeros(backward_indexes.shape[0], 1).to(backward_indexes), backward_indexes+1], dim=1)\n",
        "\n",
        "        # 1. mask_tokenを結合して並べ替える．\n",
        "        # (B, N*(1-mask_ratio)+1, dim) -> (B, N+1, dim)\n",
        "        features = torch.cat([features, self.mask_token.repeat(features.shape[0], backward_indexes.shape[1] - features.shape[1], 1)], dim=1)\n",
        "        features = take_indexes(features, backward_indexes)\n",
        "        features = features + self.pos_embedding\n",
        "\n",
        "        features = self.transformer(features)\n",
        "\n",
        "        # class tokenを除去する\n",
        "        # (B, N+1, dim) -> (B, N, dim)\n",
        "        features = features[:, 1:, :]\n",
        "\n",
        "        # 2. 画像を再構成する．\n",
        "        # (B, N, dim) -> (B, N, 3 * patch_height * patch_width)\n",
        "        patches = self.head(features)\n",
        "\n",
        "        # MAEではマスクした部分でのみ損失関数を計算するため，maskも一緒に返す\n",
        "        mask = torch.zeros_like(patches)\n",
        "        mask[:, T-1:] = 1  # cls tokenを含めていた分ずらしている\n",
        "        mask = take_indexes(mask, backward_indexes[:, 1:] - 1)\n",
        "\n",
        "        img = self.patch2img(patches)\n",
        "        mask = self.patch2img(mask)\n",
        "\n",
        "        return img, mask\n",
        "\n",
        "\n",
        "class MAE_ViT(nn.Module):\n",
        "    def __init__(self, image_size=[32, 32], patch_size=[2, 2], emb_dim=192,\n",
        "                 enc_layers=12, enc_heads=3, enc_dim_head=64, enc_mlp_dim=768,\n",
        "                 dec_layers=4, dec_heads=3, dec_dim_head=64, dec_mlp_dim=768,\n",
        "                 mask_ratio=0.75, dropout=0.):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        image_size : List[int]\n",
        "            入力画像の大きさ．\n",
        "        patch_size : List[int]\n",
        "            各パッチの大きさ．\n",
        "        emb_dim : int\n",
        "            データを埋め込む次元の数．\n",
        "        {enc/dec}_layers : int\n",
        "            Encoder / Decoderに含まれるBlockの数．\n",
        "        {enc/dec}_heads : int\n",
        "            Encoder / DecoderのMulti-Head Attentionのヘッドの数．\n",
        "        {enc/dec}_dim_head : int\n",
        "            Encoder / DecoderのMulti-Head Attentionの各ヘッドの次元数．\n",
        "        {enc/dec}_mlp_dim : int\n",
        "            Encoder / DecoderのFeed-Forward Networkの隠れ層の次元数．\n",
        "        mask_ratio : float\n",
        "            入力パッチのマスクする割合．\n",
        "        dropout : float\n",
        "            ドロップアウトの確率．\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = MAE_Encoder(image_size, patch_size, emb_dim, enc_layers,\n",
        "                                   enc_heads, enc_dim_head, enc_mlp_dim, mask_ratio, dropout)\n",
        "        self.decoder = MAE_Decoder(image_size, patch_size, emb_dim, dec_layers,\n",
        "                                   dec_heads, dec_dim_head, dec_mlp_dim, dropout)\n",
        "\n",
        "    def forward(self, img):\n",
        "        features, backward_indexes = self.encoder(img)\n",
        "        rec_img, mask = self.decoder(features, backward_indexes)\n",
        "        return rec_img, mask\n",
        "\n",
        "    def get_last_selfattention(self, x):\n",
        "        patches = self.encoder.patchify(x)\n",
        "        patches = patches + self.encoder.pos_embedding\n",
        "\n",
        "        patches = torch.cat([self.encoder.cls_token.repeat(patches.shape[0], 1, 1), patches], dim=1)  # class tokenを結合\n",
        "        for i, block in enumerate(self.encoder.transformer):\n",
        "            if i < len(self.encoder.transformer) - 1:\n",
        "                patches = block(patches)\n",
        "            else:\n",
        "                return block(patches, return_attn=True)\n",
        "\n",
        "class CosineScheduler:\n",
        "    def __init__(self, epochs, lr, warmup_length=5):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        epochs : int\n",
        "            学習のエポック数．\n",
        "        lr : float\n",
        "            学習率．\n",
        "        warmup_length : int\n",
        "            warmupを適用するエポック数．\n",
        "        \"\"\"\n",
        "        self.epochs = epochs\n",
        "        self.lr = lr\n",
        "        self.warmup = warmup_length\n",
        "\n",
        "    def __call__(self, epoch):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        epoch : int\n",
        "            現在のエポック数．\n",
        "        \"\"\"\n",
        "        progress = (epoch - self.warmup) / (self.epochs - self.warmup)\n",
        "        progress = np.clip(progress, 0.0, 1.0)\n",
        "        lr = self.lr * 0.5 * (1. + np.cos(np.pi * progress))\n",
        "\n",
        "        if self.warmup:\n",
        "            lr = lr * min(1., (epoch+1) / self.warmup)\n",
        "\n",
        "        return lr\n",
        "def set_lr(lr, optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR8uNlkCxo3d"
      },
      "source": [
        "### 事前学習（自己教師あり学習）"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/Colab Notebooks/DLBasics2023_colab/Lecture09/model/\""
      ],
      "metadata": {
        "id": "qtq2Qu7tlU_J"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"image_size\": [32, 32],\n",
        "    \"patch_size\": [2, 2],\n",
        "    \"emb_dim\": 192,\n",
        "    \"enc_layers\": 12,\n",
        "    \"enc_heads\": 3,\n",
        "    \"enc_dim_head\": 64,\n",
        "    \"enc_mlp_dim\": 192,\n",
        "    \"dec_layers\": 4,\n",
        "    \"dec_heads\": 3,\n",
        "    \"dec_dim_head\": 64,\n",
        "    \"dec_mlp_dim\": 192,\n",
        "    \"mask_ratio\": 0.75,\n",
        "    \"dropout\": 0.\n",
        "}"
      ],
      "metadata": {
        "id": "BDt9_mlTg0uP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MAE_ViT(**config).to(device)\n",
        "\n",
        "epochs = 100\n",
        "lr = 0.01\n",
        "warmup_length = 200\n",
        "batch_size = 512\n",
        "step_count = 0\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.95), weight_decay=0.05)\n",
        "scheduler = CosineScheduler(epochs, lr, warmup_length)\n",
        "early_stopping = EarlyStopping(patience=10, path=path+\"MAE_pretrain_params.pth\")\n",
        "model.load_state_dict(torch.load(path+\"MAE_pretrain_params.pth\", map_location=device))"
      ],
      "metadata": {
        "id": "u0gmReuFgv55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfIeOGbbxqXx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "717ff5eb-bf49-422a-eeb3-49deef6d48f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[1 / 100] Train Loss: 0.0082 Valid Loss: 0.0084\n",
            "Epoch[2 / 100] Train Loss: 0.0082 Valid Loss: 0.0084\n",
            "Epoch[3 / 100] Train Loss: 0.0082 Valid Loss: 0.0083\n",
            "Epoch[4 / 100] Train Loss: 0.0082 Valid Loss: 0.0083\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch[5 / 100] Train Loss: 0.0082 Valid Loss: 0.0083\n",
            "Epoch[6 / 100] Train Loss: 0.0082 Valid Loss: 0.0083\n",
            "Epoch[7 / 100] Train Loss: 0.0082 Valid Loss: 0.0084\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch[8 / 100] Train Loss: 0.0083 Valid Loss: 0.0083\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch[9 / 100] Train Loss: 0.0082 Valid Loss: 0.0084\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch[10 / 100] Train Loss: 0.0082 Valid Loss: 0.0084\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch[11 / 100] Train Loss: 0.0082 Valid Loss: 0.0084\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch[12 / 100] Train Loss: 0.0083 Valid Loss: 0.0084\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch[13 / 100] Train Loss: 0.0082 Valid Loss: 0.0084\n",
            "EarlyStopping counter: 7 out of 10\n",
            "Epoch[14 / 100] Train Loss: 0.0082 Valid Loss: 0.0084\n",
            "EarlyStopping counter: 8 out of 10\n",
            "Epoch[15 / 100] Train Loss: 0.0082 Valid Loss: 0.0083\n",
            "Epoch[16 / 100] Train Loss: 0.0082 Valid Loss: 0.0084\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch[17 / 100] Train Loss: 0.0082 Valid Loss: 0.0083\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch[18 / 100] Train Loss: 0.0082 Valid Loss: 0.0083\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch[19 / 100] Train Loss: 0.0082 Valid Loss: 0.0084\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch[20 / 100] Train Loss: 0.0082 Valid Loss: 0.0083\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch[21 / 100] Train Loss: 0.0082 Valid Loss: 0.0084\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch[22 / 100] Train Loss: 0.0082 Valid Loss: 0.0083\n",
            "EarlyStopping counter: 7 out of 10\n",
            "Epoch[23 / 100] Train Loss: 0.0082 Valid Loss: 0.0083\n",
            "Epoch[24 / 100] Train Loss: 0.0082 Valid Loss: 0.0084\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch[25 / 100] Train Loss: 0.0082 Valid Loss: 0.0083\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch[26 / 100] Train Loss: 0.0082 Valid Loss: 0.0084\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch[27 / 100] Train Loss: 0.0082 Valid Loss: 0.0084\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch[28 / 100] Train Loss: 0.0082 Valid Loss: 0.0084\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch[29 / 100] Train Loss: 0.0082 Valid Loss: 0.0083\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch[30 / 100] Train Loss: 0.0082 Valid Loss: 0.0083\n",
            "EarlyStopping counter: 7 out of 10\n",
            "Epoch[31 / 100] Train Loss: 0.0082 Valid Loss: 0.0084\n",
            "EarlyStopping counter: 8 out of 10\n",
            "Epoch[32 / 100] Train Loss: 0.0082 Valid Loss: 0.0084\n",
            "EarlyStopping counter: 9 out of 10\n",
            "Epoch[33 / 100] Train Loss: 0.0082 Valid Loss: 0.0083\n",
            "EarlyStopping counter: 10 out of 10\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    # スケジューラで学習率を更新する\n",
        "    new_lr = scheduler(epoch)\n",
        "    set_lr(new_lr, optimizer)\n",
        "\n",
        "    total_train_loss = 0.\n",
        "    total_valid_loss = 0.\n",
        "\n",
        "    # モデルの訓練\n",
        "    for x, _ in dataloader_train:\n",
        "        step_count += 1\n",
        "        model.train()\n",
        "        x = x.to(device)\n",
        "\n",
        "        rec_img, mask = model(x)\n",
        "        train_loss = torch.mean((rec_img - x) ** 2 * mask) / config[\"mask_ratio\"]\n",
        "        train_loss.backward()\n",
        "\n",
        "        if step_count % 8 == 0:  # 8イテレーションごとに更新することで，擬似的にバッチサイズを大きくしている\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        total_train_loss += train_loss.item()\n",
        "\n",
        "    # モデルの評価\n",
        "    with torch.no_grad():\n",
        "        for x, _ in dataloader_valid:\n",
        "            model.eval()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                x = x.to(device)\n",
        "\n",
        "                rec_img, mask = model(x)\n",
        "                valid_loss = torch.mean((rec_img - x) ** 2 * mask) / config[\"mask_ratio\"]\n",
        "\n",
        "                total_valid_loss += valid_loss.item()\n",
        "\n",
        "    print(f\"Epoch[{epoch+1} / {epochs}] Train Loss: {total_train_loss/len(dataloader_train):.4f} Valid Loss: {total_valid_loss/len(dataloader_valid):.4f}\")\n",
        "    early_stopping(total_valid_loss, model)\n",
        "    if early_stopping.early_stop:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHOBi4auxuPR"
      },
      "source": [
        "### Linear probing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model, train_loss, valid_loss\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "F_2IDDlFq-ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1GeuhPBryfQa"
      },
      "outputs": [],
      "source": [
        "val_size = 3000\n",
        "train_data, valid_data = torch.utils.data.random_split(trainval_data, [len(trainval_data) - val_size, val_size])\n",
        "\n",
        "train_transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize(0.5, 0.5)]\n",
        ")\n",
        "valid_transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize(0.5, 0.5)]\n",
        ")\n",
        "\n",
        "test_transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize(0.5, 0.5)]\n",
        ")\n",
        "\n",
        "train_data.transform = train_transform\n",
        "valid_data.transform = valid_transform\n",
        "test_data.transform = test_transform\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "dataloader_train = torch.utils.data.DataLoader(\n",
        "    train_data,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "dataloader_valid = torch.utils.data.DataLoader(\n",
        "    valid_data,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "dataloader_test = torch.utils.data.DataLoader(\n",
        "    test_data,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "z8n5wVT-xvv1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63bdd562-80f3-48d0-c5df-7b4c93f01127"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, encoder: MAE_Encoder, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.cls_token = encoder.cls_token\n",
        "        self.pos_embedding = encoder.pos_embedding\n",
        "        self.patchify = encoder.patchify\n",
        "        self.transformer = encoder.transformer\n",
        "        self.layer_norm = encoder.layer_norm\n",
        "        self.head = nn.Linear(self.pos_embedding.shape[-1], num_classes)\n",
        "\n",
        "    def forward(self, img):\n",
        "        patches = self.patchify(img)\n",
        "        patches = patches + self.pos_embedding  # positional embedding\n",
        "\n",
        "        patches = torch.cat([self.cls_token.repeat(patches.shape[0], 1, 1), patches], dim=1)  # class tokenを結合\n",
        "        features = self.layer_norm(self.transformer(patches))\n",
        "        logits = self.head(features[:, 0])  # cls tokenのみを入力する\n",
        "        return logits\n",
        "\n",
        "    def get_last_selfattention(self, x):\n",
        "        patches = self.patchify(x)\n",
        "        patches = patches + self.pos_embedding\n",
        "\n",
        "        patches = torch.cat([self.cls_token.repeat(patches.shape[0], 1, 1), patches], dim=1)  # class tokenを結合\n",
        "        for i, block in enumerate(self.transformer):\n",
        "            if i < len(self.transformer) - 1:\n",
        "                patches = block(patches)\n",
        "            else:\n",
        "                return block(patches, return_attn=True)\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model = MAE_ViT(**config).to(device)\n",
        "pretrained_model.load_state_dict(torch.load(path+\"MAE_pretrain_params.pth\", map_location=device))\n",
        "encoder = pretrained_model.encoder\n",
        "\n",
        "# モデルの定義\n",
        "model = Classifier(encoder).to(device)\n",
        "\n",
        "epochs = 100\n",
        "lr = 0.001\n",
        "warmup_length = 5\n",
        "optimizer = optim.AdamW(model.head.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=0.05)  # 分類器部分のみ学習\n",
        "scheduler = CosineScheduler(epochs, lr, warmup_length)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "early_stopping = EarlyStopping(patience=10, path=path+\"MAE_classifier_params.pth\")\n",
        "model.load_state_dict(torch.load(path+\"MAE_classifier_params.pth\", map_location=device))"
      ],
      "metadata": {
        "id": "oWM7Uj2MrR7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "520167b9-3a92-4fd9-a0a2-45900fa8feb3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5T6QiHwyTRj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "605cf36a-8393-4b29-e8a9-5d35e11a4cd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[1 / 100] Train Loss: 1.1910 Train Acc.: 0.6293 Valid Loss: 1.2052 Valid Acc.: 0.6170\n",
            "Epoch[2 / 100] Train Loss: 1.1630 Train Acc.: 0.6330 Valid Loss: 1.1738 Valid Acc.: 0.6211\n",
            "Epoch[3 / 100] Train Loss: 1.1267 Train Acc.: 0.6381 Valid Loss: 1.1333 Valid Acc.: 0.6263\n",
            "Epoch[4 / 100] Train Loss: 1.0904 Train Acc.: 0.6438 Valid Loss: 1.0957 Valid Acc.: 0.6373\n",
            "Epoch[5 / 100] Train Loss: 1.0563 Train Acc.: 0.6511 Valid Loss: 1.0659 Valid Acc.: 0.6468\n",
            "Epoch[6 / 100] Train Loss: 1.0286 Train Acc.: 0.6573 Valid Loss: 1.0440 Valid Acc.: 0.6523\n",
            "Epoch[7 / 100] Train Loss: 1.0083 Train Acc.: 0.6619 Valid Loss: 1.0255 Valid Acc.: 0.6543\n",
            "Epoch[8 / 100] Train Loss: 0.9913 Train Acc.: 0.6668 Valid Loss: 1.0169 Valid Acc.: 0.6555\n",
            "Epoch[9 / 100] Train Loss: 0.9779 Train Acc.: 0.6704 Valid Loss: 0.9982 Valid Acc.: 0.6599\n",
            "Epoch[10 / 100] Train Loss: 0.9680 Train Acc.: 0.6728 Valid Loss: 0.9859 Valid Acc.: 0.6651\n",
            "Epoch[11 / 100] Train Loss: 0.9568 Train Acc.: 0.6764 Valid Loss: 0.9757 Valid Acc.: 0.6738\n",
            "Epoch[12 / 100] Train Loss: 0.9484 Train Acc.: 0.6795 Valid Loss: 0.9784 Valid Acc.: 0.6652\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch[13 / 100] Train Loss: 0.9413 Train Acc.: 0.6813 Valid Loss: 0.9652 Valid Acc.: 0.6753\n",
            "Epoch[14 / 100] Train Loss: 0.9351 Train Acc.: 0.6824 Valid Loss: 0.9616 Valid Acc.: 0.6743\n",
            "Epoch[15 / 100] Train Loss: 0.9298 Train Acc.: 0.6852 Valid Loss: 0.9548 Valid Acc.: 0.6774\n",
            "Epoch[16 / 100] Train Loss: 0.9239 Train Acc.: 0.6864 Valid Loss: 0.9522 Valid Acc.: 0.6755\n",
            "Epoch[17 / 100] Train Loss: 0.9192 Train Acc.: 0.6878 Valid Loss: 0.9515 Valid Acc.: 0.6774\n",
            "Epoch[18 / 100] Train Loss: 0.9148 Train Acc.: 0.6888 Valid Loss: 0.9444 Valid Acc.: 0.6831\n",
            "Epoch[19 / 100] Train Loss: 0.9122 Train Acc.: 0.6902 Valid Loss: 0.9385 Valid Acc.: 0.6798\n",
            "Epoch[20 / 100] Train Loss: 0.9070 Train Acc.: 0.6911 Valid Loss: 0.9402 Valid Acc.: 0.6815\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch[21 / 100] Train Loss: 0.9050 Train Acc.: 0.6922 Valid Loss: 0.9314 Valid Acc.: 0.6821\n",
            "Epoch[22 / 100] Train Loss: 0.9014 Train Acc.: 0.6933 Valid Loss: 0.9279 Valid Acc.: 0.6823\n",
            "Epoch[23 / 100] Train Loss: 0.8991 Train Acc.: 0.6941 Valid Loss: 0.9257 Valid Acc.: 0.6852\n",
            "Epoch[24 / 100] Train Loss: 0.8958 Train Acc.: 0.6944 Valid Loss: 0.9312 Valid Acc.: 0.6808\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch[25 / 100] Train Loss: 0.8938 Train Acc.: 0.6961 Valid Loss: 0.9248 Valid Acc.: 0.6845\n",
            "Epoch[26 / 100] Train Loss: 0.8922 Train Acc.: 0.6958 Valid Loss: 0.9199 Valid Acc.: 0.6851\n",
            "Epoch[27 / 100] Train Loss: 0.8895 Train Acc.: 0.6969 Valid Loss: 0.9188 Valid Acc.: 0.6878\n",
            "Epoch[28 / 100] Train Loss: 0.8885 Train Acc.: 0.6974 Valid Loss: 0.9198 Valid Acc.: 0.6850\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch[29 / 100] Train Loss: 0.8863 Train Acc.: 0.6980 Valid Loss: 0.9139 Valid Acc.: 0.6869\n",
            "Epoch[30 / 100] Train Loss: 0.8852 Train Acc.: 0.6993 Valid Loss: 0.9117 Valid Acc.: 0.6886\n",
            "Epoch[31 / 100] Train Loss: 0.8825 Train Acc.: 0.7001 Valid Loss: 0.9150 Valid Acc.: 0.6857\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch[32 / 100] Train Loss: 0.8819 Train Acc.: 0.6992 Valid Loss: 0.9111 Valid Acc.: 0.6862\n",
            "Epoch[33 / 100] Train Loss: 0.8806 Train Acc.: 0.7010 Valid Loss: 0.9064 Valid Acc.: 0.6885\n",
            "Epoch[34 / 100] Train Loss: 0.8793 Train Acc.: 0.7005 Valid Loss: 0.9074 Valid Acc.: 0.6895\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch[35 / 100] Train Loss: 0.8777 Train Acc.: 0.7006 Valid Loss: 0.9065 Valid Acc.: 0.6872\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch[36 / 100] Train Loss: 0.8763 Train Acc.: 0.7017 Valid Loss: 0.9065 Valid Acc.: 0.6910\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch[37 / 100] Train Loss: 0.8754 Train Acc.: 0.7019 Valid Loss: 0.9027 Valid Acc.: 0.6904\n",
            "Epoch[38 / 100] Train Loss: 0.8749 Train Acc.: 0.7019 Valid Loss: 0.9070 Valid Acc.: 0.6904\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch[39 / 100] Train Loss: 0.8734 Train Acc.: 0.7021 Valid Loss: 0.9026 Valid Acc.: 0.6922\n",
            "Epoch[40 / 100] Train Loss: 0.8733 Train Acc.: 0.7026 Valid Loss: 0.9063 Valid Acc.: 0.6869\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch[41 / 100] Train Loss: 0.8713 Train Acc.: 0.7033 Valid Loss: 0.9013 Valid Acc.: 0.6907\n",
            "Epoch[42 / 100] Train Loss: 0.8706 Train Acc.: 0.7042 Valid Loss: 0.8961 Valid Acc.: 0.6963\n",
            "Epoch[43 / 100] Train Loss: 0.8704 Train Acc.: 0.7037 Valid Loss: 0.8976 Valid Acc.: 0.6950\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch[44 / 100] Train Loss: 0.8696 Train Acc.: 0.7037 Valid Loss: 0.8998 Valid Acc.: 0.6937\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch[45 / 100] Train Loss: 0.8683 Train Acc.: 0.7049 Valid Loss: 0.9047 Valid Acc.: 0.6879\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch[46 / 100] Train Loss: 0.8682 Train Acc.: 0.7046 Valid Loss: 0.8991 Valid Acc.: 0.6938\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch[47 / 100] Train Loss: 0.8671 Train Acc.: 0.7049 Valid Loss: 0.9002 Valid Acc.: 0.6934\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch[48 / 100] Train Loss: 0.8668 Train Acc.: 0.7054 Valid Loss: 0.8941 Valid Acc.: 0.6911\n",
            "Epoch[49 / 100] Train Loss: 0.8665 Train Acc.: 0.7046 Valid Loss: 0.8997 Valid Acc.: 0.6909\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch[50 / 100] Train Loss: 0.8657 Train Acc.: 0.7057 Valid Loss: 0.8981 Valid Acc.: 0.6951\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch[51 / 100] Train Loss: 0.8654 Train Acc.: 0.7059 Valid Loss: 0.8977 Valid Acc.: 0.6947\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch[52 / 100] Train Loss: 0.8649 Train Acc.: 0.7063 Valid Loss: 0.8976 Valid Acc.: 0.6917\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch[53 / 100] Train Loss: 0.8651 Train Acc.: 0.7057 Valid Loss: 0.8959 Valid Acc.: 0.6946\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch[54 / 100] Train Loss: 0.8642 Train Acc.: 0.7061 Valid Loss: 0.8952 Valid Acc.: 0.6908\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch[55 / 100] Train Loss: 0.8647 Train Acc.: 0.7060 Valid Loss: 0.8947 Valid Acc.: 0.6936\n",
            "EarlyStopping counter: 7 out of 10\n",
            "Epoch[56 / 100] Train Loss: 0.8631 Train Acc.: 0.7061 Valid Loss: 0.8900 Valid Acc.: 0.6929\n",
            "Epoch[57 / 100] Train Loss: 0.8623 Train Acc.: 0.7068 Valid Loss: 0.8954 Valid Acc.: 0.6887\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch[58 / 100] Train Loss: 0.8635 Train Acc.: 0.7063 Valid Loss: 0.8907 Valid Acc.: 0.6933\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch[59 / 100] Train Loss: 0.8618 Train Acc.: 0.7068 Valid Loss: 0.8926 Valid Acc.: 0.6941\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch[60 / 100] Train Loss: 0.8617 Train Acc.: 0.7072 Valid Loss: 0.8960 Valid Acc.: 0.6902\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch[61 / 100] Train Loss: 0.8619 Train Acc.: 0.7074 Valid Loss: 0.8974 Valid Acc.: 0.6922\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch[62 / 100] Train Loss: 0.8607 Train Acc.: 0.7069 Valid Loss: 0.8976 Valid Acc.: 0.6929\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch[63 / 100] Train Loss: 0.8612 Train Acc.: 0.7071 Valid Loss: 0.8903 Valid Acc.: 0.6914\n",
            "EarlyStopping counter: 7 out of 10\n",
            "Epoch[64 / 100] Train Loss: 0.8612 Train Acc.: 0.7084 Valid Loss: 0.8932 Valid Acc.: 0.6918\n",
            "EarlyStopping counter: 8 out of 10\n",
            "Epoch[65 / 100] Train Loss: 0.8610 Train Acc.: 0.7080 Valid Loss: 0.8890 Valid Acc.: 0.6942\n",
            "Epoch[66 / 100] Train Loss: 0.8603 Train Acc.: 0.7075 Valid Loss: 0.8918 Valid Acc.: 0.6930\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch[67 / 100] Train Loss: 0.8603 Train Acc.: 0.7077 Valid Loss: 0.8893 Valid Acc.: 0.6916\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch[68 / 100] Train Loss: 0.8605 Train Acc.: 0.7074 Valid Loss: 0.8927 Valid Acc.: 0.6903\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch[69 / 100] Train Loss: 0.8598 Train Acc.: 0.7082 Valid Loss: 0.8944 Valid Acc.: 0.6953\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch[70 / 100] Train Loss: 0.8596 Train Acc.: 0.7080 Valid Loss: 0.8884 Valid Acc.: 0.6961\n",
            "Epoch[71 / 100] Train Loss: 0.8592 Train Acc.: 0.7078 Valid Loss: 0.8932 Valid Acc.: 0.6909\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch[72 / 100] Train Loss: 0.8591 Train Acc.: 0.7076 Valid Loss: 0.8918 Valid Acc.: 0.6921\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch[73 / 100] Train Loss: 0.8593 Train Acc.: 0.7078 Valid Loss: 0.8912 Valid Acc.: 0.6937\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch[74 / 100] Train Loss: 0.8586 Train Acc.: 0.7080 Valid Loss: 0.8920 Valid Acc.: 0.6959\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch[75 / 100] Train Loss: 0.8591 Train Acc.: 0.7076 Valid Loss: 0.8903 Valid Acc.: 0.6939\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch[76 / 100] Train Loss: 0.8592 Train Acc.: 0.7083 Valid Loss: 0.8915 Valid Acc.: 0.6958\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch[77 / 100] Train Loss: 0.8588 Train Acc.: 0.7081 Valid Loss: 0.8898 Valid Acc.: 0.6962\n",
            "EarlyStopping counter: 7 out of 10\n",
            "Epoch[78 / 100] Train Loss: 0.8590 Train Acc.: 0.7083 Valid Loss: 0.8923 Valid Acc.: 0.6931\n",
            "EarlyStopping counter: 8 out of 10\n",
            "Epoch[79 / 100] Train Loss: 0.8590 Train Acc.: 0.7086 Valid Loss: 0.8861 Valid Acc.: 0.6939\n",
            "Epoch[80 / 100] Train Loss: 0.8588 Train Acc.: 0.7081 Valid Loss: 0.8880 Valid Acc.: 0.6947\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch[81 / 100] Train Loss: 0.8578 Train Acc.: 0.7091 Valid Loss: 0.8893 Valid Acc.: 0.6922\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch[82 / 100] Train Loss: 0.8577 Train Acc.: 0.7088 Valid Loss: 0.8873 Valid Acc.: 0.6938\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch[83 / 100] Train Loss: 0.8582 Train Acc.: 0.7085 Valid Loss: 0.8898 Valid Acc.: 0.6930\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch[84 / 100] Train Loss: 0.8580 Train Acc.: 0.7082 Valid Loss: 0.8948 Valid Acc.: 0.6922\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch[85 / 100] Train Loss: 0.8574 Train Acc.: 0.7088 Valid Loss: 0.8868 Valid Acc.: 0.6949\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch[86 / 100] Train Loss: 0.8585 Train Acc.: 0.7089 Valid Loss: 0.8891 Valid Acc.: 0.6930\n",
            "EarlyStopping counter: 7 out of 10\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    new_lr = scheduler(epoch)\n",
        "    set_lr(new_lr, optimizer)\n",
        "\n",
        "    total_train_loss = 0.\n",
        "    total_train_acc = 0.\n",
        "    total_valid_loss = 0.\n",
        "    total_valid_acc = 0.\n",
        "    for x, t in dataloader_train:\n",
        "        x, t = x.to(device), t.to(device)\n",
        "        pred = model(x)\n",
        "\n",
        "        train_loss = criterion(pred, t)\n",
        "        train_acc = (torch.argmax(pred, dim=1) == t).float().mean().cpu()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += train_loss.item()\n",
        "        total_train_acc += train_acc\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, t in dataloader_valid:\n",
        "            x, t = x.to(device), t.to(device)\n",
        "            pred = model(x)\n",
        "\n",
        "            valid_loss = criterion(pred, t)\n",
        "            valid_acc = (torch.argmax(pred, dim=1) == t).float().mean().cpu()\n",
        "\n",
        "            total_valid_loss += valid_loss.item()\n",
        "            total_valid_acc += valid_acc\n",
        "\n",
        "    print(f\"Epoch[{epoch+1} / {epochs}]\",\n",
        "          f\"Train Loss: {total_train_loss/len(dataloader_train):.4f}\",\n",
        "          f\"Train Acc.: {total_train_acc/len(dataloader_train):.4f}\",\n",
        "          f\"Valid Loss: {total_valid_loss/len(dataloader_valid):.4f}\",\n",
        "          f\"Valid Acc.: {total_valid_acc/len(dataloader_valid):.4f}\")\n",
        "    early_stopping(total_valid_loss, model)\n",
        "    if early_stopping.early_stop:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "72n19Q_RyqWl"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "t_pred = []\n",
        "for x in dataloader_test:\n",
        "    x = x.to(device)\n",
        "    y = model(x)\n",
        "\n",
        "    # モデルの出力を予測値のスカラーに変換\n",
        "    pred = y.argmax(1).tolist()\n",
        "    t_pred.extend(pred)\n",
        "\n",
        "submission = pd.Series(t_pred, name='label')\n",
        "submission.to_csv(\"drive/MyDrive/Colab Notebooks/DLBasics2023_colab/Lecture09/submission_pred.csv\", header=True, index_label='id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwKJ8IBcy5TG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}